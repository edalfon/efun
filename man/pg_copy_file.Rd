% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/postgres.R
\name{pg_copy_file}
\alias{pg_copy_file}
\title{Copy data directly from a delimited text-file (csv) to a table in PostgreSQL}
\usage{
pg_copy_file(
  con,
  file_path,
  table,
  sep = ";",
  nrows = 10000,
  header = TRUE,
  na.strings = c("", "NA", "NULL"),
  colClasses = NULL,
  encoding = "UTF-8",
  ...,
  unlogged = FALSE,
  drop_table = FALSE,
  cascade = FALSE,
  if_not_exists = FALSE,
  create_table = FALSE,
  execute = FALSE,
  quote = "\\b",
  escape = quote,
  null = "NULL"
)
}
\arguments{
\item{con}{A database connection.}

\item{file_path}{path to a text file (csv, tab-delimited, etc.)}

\item{table}{Name of the table. Escaped with
\code{\link[DBI:dbQuoteIdentifier]{dbQuoteIdentifier()}}.}

\item{sep}{ The separator between columns. Defaults to the character in the set \code{[,\\t |;:]} that separates the sample of rows into the most number of lines with the same number of fields. Use \code{NULL} or \code{""} to specify no separator; i.e. each line a single character column like \code{base::readLines} does.}

\item{nrows}{ The maximum number of rows to read. Unlike \code{read.table}, you do not need to set this to an estimate of the number of rows in the file for better speed because that is already automatically determined by \code{fread} almost instantly using the large sample of lines. \code{nrows=0} returns the column names and typed empty columns determined by the large sample; useful for a dry run of a large file or to quickly check format consistency of a set of files before starting to read any of them. }

\item{header}{ Does the first data line contain column names? Defaults according to whether every non-empty field on the first data line is type character. If so, or TRUE is supplied, any empty column names are given a default name. }

\item{na.strings}{ A character vector of strings which are to be interpreted as \code{NA} values. By default, \code{",,"} for columns of all types, including type \code{character} is read as \code{NA} for consistency. \code{,"",} is unambiguous and read as an empty string. To read \code{,NA,} as \code{NA}, set \code{na.strings="NA"}. To read \code{,,} as blank string \code{""}, set \code{na.strings=NULL}. When they occur in the file, the strings in \code{na.strings} should not appear quoted since that is how the string literal \code{,"NA",} is distinguished from \code{,NA,}, for example, when \code{na.strings="NA"}. }

\item{colClasses}{ As in \code{\link[utils:read.table]{utils::read.csv}}; i.e., an unnamed vector of types corresponding to the columns in the file, or a named vector specifying types for a subset of the columns by name. The default, \code{NULL} means types are inferred from the data in the file. Further, \code{data.table} supports a named \code{list} of vectors of column names \emph{or numbers} where the \code{list} names are the class names; see examples. The \code{list} form makes it easier to set a batch of columns to be a particular class. When column numbers are used in the \code{list} form, they refer to the column number in the file not the column number after \code{select} or \code{drop} has been applied.
    If type coercion results in an error, introduces \code{NA}s, or would result in loss of accuracy, the coercion attempt is aborted for that column with warning and the column's type is left unchanged. If you really desire data loss (e.g. reading \code{3.14} as \code{integer}) you have to truncate such columns afterwards yourself explicitly so that this is clear to future readers of your code.
  }

\item{encoding}{ default is \code{"unknown"}. Other possible options are \code{"UTF-8"} and \code{"Latin-1"}.  Note: it is not used to re-encode the input, rather enables handling of encoded strings in their native encoding. }

\item{...}{Other arguments used by individual methods.}

\item{unlogged}{logical, whether to create an UNLOGGED table}

\item{drop_table}{logical, whether to drop the table before creating it}

\item{cascade}{logical, whether to add CASCADE to the DROP statement}

\item{if_not_exists}{logical, to add IF NOT EXISTS to the query}

\item{create_table}{boolean TRUE if the table should be created. Otherwise,
it assumes the table exists}

\item{execute}{logical, whether to execute the query using \code{con}}

\item{quote}{"quoting character to be used when a data value is quoted"
the default value is just a dirty-little-trick to use a value very
unlikely to appear}

\item{escape}{"character that should appear before a data character that
matches the QUOTE value"}

\item{null}{"Specifies the string that represents a null value."}
}
\value{
a dplyr reference to the table
}
\description{
This is a convenience function meant to be used to import data into a
locally-run PostgreSQL server. It basically uses PostgreSQL's COPY command
to import the data and by default, it automates the creation of the table
taking column names from the header (first row) of the file and guessing
column types ala ?data.table::fread
}
\details{
This function uses PostgreSQL's COPY FROM filename command.
https://www.postgresql.org/docs/current/static/sql-copy.html.
According to the documentation, "The file must be accessible to the server
and the name must be specified from the viewpoint of the server". Thererfore,
this function is intended to be used when you run the PostgreSQL server
locally (if you manage to put the file in the server machine, you could also
use it, though).
\itemize{
\item Use UNLOGGED tables and, if possible, create the table and copy command
within the same transaction (to improve performance).
\href{https://nbsoftsolutions.com/blog/disecting-the-postgres-bulk-insert-and-binary-format}{See some discussion here}
\item TODO: use also the COPY FROM PROGRAM syntax, to import directly from zip
files \url{https://www.postgresql.org/docs/current/static/sql-copy.html}
\item TODO: also perhaps use named pipes to read from zip files directly
http://www.ralree.com/2009/09/04/reading-compressed-files-with-postgres-
using-named-pipes/
https://stackoverflow.com/questions/41738829/importing-zipped-csv-file-into-postgresql
}
}
